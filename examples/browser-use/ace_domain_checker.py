#!/usr/bin/env python3
"""
ACE + Browser-Use Domain Checker Demo

Simple demo showing ACE learning to improve at checking domain availability.
Uses OnlineAdapter for incremental learning after each domain check.
"""

import asyncio
from pathlib import Path
from typing import List
from dotenv import load_dotenv
from ace.prompts_v2 import PromptManager
from browser_use import Agent, Browser, ChatOpenAI

from ace import (
    LiteLLMClient,
    Generator,
    Reflector,
    Curator,
    OnlineAdapter,
    Sample,
    TaskEnvironment,
    EnvironmentResult,
    Playbook,
)
from ace.observability import configure_opik

load_dotenv()


class DomainCheckEnvironment(TaskEnvironment):
    """Environment that evaluates domain checking performance."""

    def __init__(self, headless: bool = True, model: str = "gpt-4o"):
        self.headless = headless
        self.model = model

    def evaluate(self, sample: Sample, generator_output):
        """Run browser automation and evaluate the result."""

        # Extract domain from the sample question
        domain = sample.question.replace("Check if domain ", "").replace(" is available", "")

        # Get strategy from generator
        strategy = generator_output.final_answer

        print(f"ğŸ” Checking domain: {domain}")

        # Run browser automation
        result = asyncio.run(self._check_domain(domain, strategy))

        # Evaluate correctness and efficiency
        status_success = result['status'] != "ERROR"

        # For testing purposes, assume test domains should be AVAILABLE
        expected_status = "AVAILABLE"
        correct = (result['status'] == expected_status) if status_success else False
        efficient = result['steps'] <= 8  # Simple threshold for feedback context

        feedback = f"Domain check {'succeeded' if status_success else 'failed'}. "
        feedback += f"Took {result['steps']} steps. "

        if status_success:
            if result['status'] == expected_status:
                feedback += f"Correctly identified domain as {result['status']}. "
            else:
                feedback += f"Incorrectly identified domain as {result['status']} (expected: {expected_status}). "

            if correct and not efficient:
                feedback += f"Analyze what made this attempt take more steps (target: â‰¤8 steps). "
            elif correct:
                feedback += f"Analyze what made this attempt efficient. "
        else:
            feedback += f"Error: {result.get('error', 'Unknown error')}. "


        return EnvironmentResult(
            feedback=feedback,
            ground_truth=None,  # No ground truth available for domain checking
            metrics={
                "correct": correct,
                "status_success": status_success,
                "efficient": efficient,
                "steps": result['steps'],
                "total_steps": result.get('total_steps', result['steps']),
                "status": result['status'],
                "expected": expected_status,
                "attempt": result.get('attempt', 1),
                "attempt_details": result.get('attempt_details', []),
                "browseruse_tokens": 0,
                "browseruse_cost": 0.0,
                "ace_tokens": 0,
                "ace_cost": 0.0
            }
        )

    async def _check_domain(self, domain: str, strategy: str):
        """Execute browser automation to check domain with retry logic."""
        max_retries = 3
        last_error = None
        total_steps = 0
        attempt_details = []


        for attempt in range(max_retries):
            print(f"   â³ Attempt {attempt + 1}/{max_retries}...")
            browser = None
            try:
                # Start browser
                browser = Browser(headless=self.headless)
                await browser.start()

                # Create agent with the strategy
                llm = ChatOpenAI(model=self.model, temperature=0.0)

                task = f"""
You are a domain availability checking agent. Check if the domain "{domain}" is available.

  IMPORTANT: Do NOT navigate to {domain} directly. Instead:
  1. Go to a domain checking website
  2. In the search bar type "{domain}" on that website
  3. Read the availability status from the results

Output format (exactly one of these):
AVAILABLE: {domain}
TAKEN: {domain}
ERROR: <reason>

{strategy}"""

                agent = Agent(
                    task=task,
                    llm=llm,
                    browser=browser,
                    max_actions_per_step=5,
                    max_steps=20,
                )

                # Run with timeout
                history = await asyncio.wait_for(agent.run(), timeout=180.0)

                # Parse result
                output = history.final_result() if hasattr(history, "final_result") else ""
                steps = len(history.action_names()) if hasattr(history, "action_names") and history.action_names() else 0

                # Add steps to total and track attempt
                total_steps += steps
                attempt_details.append(f"attempt {attempt + 1}: {steps} steps")

                # Determine status
                status = "ERROR"
                output_upper = output.upper()
                domain_upper = domain.upper()

                if f"AVAILABLE: {domain_upper}" in output_upper:
                    status = "AVAILABLE"
                elif f"TAKEN: {domain_upper}" in output_upper:
                    status = "TAKEN"

                # If successful, return immediately with cumulative data
                if status != "ERROR":
                    print(f"   âœ… Success! {status} ({steps} steps)")
                    return {
                        "status": status,
                        "steps": steps,  # Steps from final attempt
                        "total_steps": total_steps,  # Cumulative steps
                        "output": output,
                        "attempt": attempt + 1,
                        "attempt_details": attempt_details,
                        "browseruse_tokens": 0,
                        "browseruse_cost": 0.0
                    }

                # Store error for potential retry
                print(f"   âŒ Failed ({steps} steps) - retrying...")
                last_error = f"Failed to get valid result: {output}"

            except asyncio.TimeoutError:
                # Get actual steps even on timeout
                try:
                    steps = history.number_of_steps() if 'history' in locals() and hasattr(history, "number_of_steps") else 0
                except:
                    steps = 20  # max_steps if we can't determine

                total_steps += steps
                attempt_details.append(f"attempt {attempt + 1}: {steps} steps (timeout)")
                print(f"   â±ï¸ Timeout ({steps} steps) - retrying...")
                last_error = f"Timeout on attempt {attempt + 1}"

            except Exception as e:
                # Get actual steps even on error
                try:
                    steps = history.number_of_steps() if 'history' in locals() and hasattr(history, "number_of_steps") else 0
                except:
                    steps = 0

                total_steps += steps
                attempt_details.append(f"attempt {attempt + 1}: {steps} steps (error)")
                print(f"   ğŸ’¥ Error ({steps} steps) - retrying...")
                last_error = f"Error on attempt {attempt + 1}: {str(e)}"

            finally:
                if browser:
                    try:
                        await browser.stop()
                    except:
                        pass

        # All retries failed
        return {
            "status": "ERROR",
            "steps": steps if 'steps' in locals() else 0,
            "total_steps": total_steps,
            "error": f"Failed after {max_retries} attempts. Last error: {last_error}",
            "attempt": max_retries,
            "attempt_details": attempt_details,
            "browseruse_tokens": 0,
            "browseruse_cost": 0.0
        }


def get_test_domains() -> List[str]:
    """Get list of test domains to check."""
    return [
        "testdomain123456.com",
        #"myuniquedomain789.net",
        #"brandnewstartup2024.io",
        #"innovativetech555.org",
        #"creativesolutions999.co",
        #"digitalagency2024.biz",
        #"techstartup123.app",
        #"newcompany456.info",
        #"uniquebusiness789.online",
        #"moderntech2024.dev"
    ]


def main():
    """Main function - ACE online learning for domain checking."""

    # Configure Opik if available
    try:
        configure_opik(project_name="ace-browser-domain-checker")
        print("ğŸ“Š Opik observability enabled")
    except:
        print("ğŸ“Š Opik not available, continuing without observability")

    print("\nğŸš€ ACE + Browser-Use Domain Checker")
    print("ğŸ§  Learns after each domain check!")
    print("=" * 50)

    # Get test domains
    domains = get_test_domains()
    print(f"ğŸ“‹ Testing {len(domains)} domains:")
    for i, domain in enumerate(domains, 1):
        print(f"  {i}. {domain}")

    # Create ACE components with OnlineAdapter
    llm = LiteLLMClient(model="gpt-4o", temperature=0.7)

    # Create prompt manager
    manager = PromptManager()

    adapter = OnlineAdapter(
        playbook=Playbook(),
        generator=Generator(llm, prompt_template=manager.get_generator_prompt()),
        reflector=Reflector(llm, prompt_template=manager.get_reflector_prompt()),
        curator=Curator(llm, prompt_template=manager.get_curator_prompt()),
        max_refinement_rounds=2,
    )

    # Create environment
    environment = DomainCheckEnvironment(
        headless=False,  # Change to True for headless mode
        model="gpt-4o"
    )

    print("\nğŸ”„ Starting incremental ACE learning...\n")

    # Create all samples
    samples = []
    for i, domain in enumerate(domains, 1):
        samples.append(Sample(
            question=f"Check if domain {domain} is available",
            ground_truth="AVAILABLE or TAKEN",
            context="Use domain lookup websites efficiently. Avoid CAPTCHAs."
        ))

    # Run OnlineAdapter - it processes samples one by one and learns after each!
    print(f"\nğŸ“‹ Processing {len(domains)} domains...")
    results = adapter.run(samples, environment)

    # Show results
    print("\n" + "=" * 80)
    print("ğŸ“Š RESULTS")
    print("=" * 80)
    print(f"{'#':<3} {'Domain':<25} {'Status':<10} {'Acc':<4} {'Steps':<8} {'Agent-Tokens':<12} {'ACE-Tokens':<11} {'Details'}")
    print("-" * 103)

    for i, (domain, result) in enumerate(zip(domains, results), 1):
        metrics = result.environment_result.metrics
        status = metrics.get('status', 'UNKNOWN')
        steps = metrics.get('steps', 0)
        total_steps = metrics.get('total_steps', steps)
        correct = metrics.get('correct', False)
        attempt = metrics.get('attempt', 1)
        attempt_details = metrics.get('attempt_details', [])

        # Show detailed step breakdown for multiple attempts
        if attempt > 1:
            step_details = f"({', '.join(attempt_details)})"
        else:
            step_details = "(1 attempt)"

        accuracy_indicator = 'âœ“' if correct else 'âœ—'
        browseruse_tokens = metrics.get('browseruse_tokens', 0)
        ace_tokens = metrics.get('ace_tokens', 0)

        print(f"{i:<3} {domain:<25} {status:<10} {accuracy_indicator:<4} {total_steps:<8} {browseruse_tokens:<12} {ace_tokens:<11} {step_details}")

    # Enhanced Summary
    status_successful = sum(1 for r in results if r.environment_result.metrics.get('status_success', False))
    correct = sum(1 for r in results if r.environment_result.metrics.get('correct', False))
    total_steps = sum(r.environment_result.metrics.get('total_steps', r.environment_result.metrics.get('steps', 0)) for r in results)
    domains_with_retries = sum(1 for r in results if r.environment_result.metrics.get('attempt', 1) > 1)
    total_attempts = sum(r.environment_result.metrics.get('attempt', 1) for r in results)

    avg_steps_per_domain = total_steps / len(results) if results else 0

    # Token/cost placeholders (always 0)
    total_browseruse_tokens = 0
    total_browseruse_cost = 0.0
    total_ace_tokens = 0
    total_ace_cost = 0.0
    avg_browseruse_tokens_per_domain = 0.0
    avg_browseruse_cost_per_domain = 0.0
    avg_ace_tokens_per_domain = 0.0
    avg_ace_cost_per_domain = 0.0

    print("\n" + "=" * 80)
    print("ğŸ“ˆ SUMMARY")
    print("=" * 80)
    print(f"âœ… Success rate:          {status_successful:>2}/{len(results)} ({100*status_successful/len(results):>5.1f}%)")
    print(f"ğŸ¯ Accuracy rate:         {correct:>2}/{len(results)} ({100*correct/len(results):>5.1f}%)")
    print(f"ğŸ”„ Domains w/ retries:    {domains_with_retries:>2}/{len(results)}")
    print(f"ğŸ”¢ Total attempts:        {total_attempts:>6}")
    print()
    print(f"{'ğŸ“Š Steps:':<20} {total_steps:>6} total     {avg_steps_per_domain:>6.1f} per domain")
    print(f"{'ğŸ¤– Agent-Tokens:':<20} {total_browseruse_tokens:>6} total     {avg_browseruse_tokens_per_domain:>6.1f} per domain")
    print(f"{'ğŸ§  ACE-Tokens:':<20} {total_ace_tokens:>6} total     {avg_ace_tokens_per_domain:>6.1f} per domain")
    print(f"{'ğŸ’° Agent-Cost:':<20} ${total_browseruse_cost:>5.4f} total      ${avg_browseruse_cost_per_domain:>5.4f} per domain")
    print(f"{'ğŸ¯ ACE-Cost:':<20} ${total_ace_cost:>5.4f} total      ${avg_ace_cost_per_domain:>5.4f} per domain")
    print("=" * 80)

    # Show learned strategies
    if adapter.playbook.bullets():
        print(f"\nğŸ¯ Learned Strategies:")
        for i, bullet in enumerate(adapter.playbook.bullets(), 1):
            print(f"  {i}. {bullet.content}")

    # Save playbook
    playbook_path = Path("ace_domain_playbook.json")
    adapter.playbook.save_to_file(str(playbook_path))
    print(f"\nğŸ’¾ Playbook saved to {playbook_path}")


if __name__ == "__main__":
    main()
